# Targeting and Retaining Customers

## R Packages and Datasets for Topic 4

``` {r t4packagesdata, message=FALSE}
library(ggplot2)       # Advanced graphing capabilities
library(tidyr)         # Easier programming 
library(GGally)        # Scatterplot matrix
library(flextable)     # Better HTML Tables
library(jtools)        # Concise regression results
library(dplyr)         # Easier programming
library(vtable)        # Nice summary statistics tables
library(cowplot)       # Arrange plots in grid
library(MKT4320BGSU)
data(bankmktg)
data(telecom)
```

## Targeting Customers

* One-to-One Marketing
    * Time consuming
    * Costly
* Mass Marketing
    * Customer needs not being met
* Target Marketing
    * Market to those likely to...

### Goal

Target customers with the highest likelihood of a favorable outcome using explanatory variables

* Outcome variable could be:
    * Purchase
    * Sales
    * Costs
    * Profitability
    * CLV
* Explanatory variables could be:
    * Demographics
    * Behaviors
    * Usage
    * Lifestyles
    
The outcome variable will dictate the type of analysis we can perform

* Continuous outcome variables have a meaningful magnitude
    * Use linear regression
* Categorical outcome variables do not have a meaningful magnitude
    * Use logistic regression
    
## Retaining Customers

Importance of retention:

> Reducing defections $5\%$ boosts profits $25\%$ to $85\%$.  â€” Frederick F. Reichheld and W. Earl Sasser, Jr.

### Goal

Identify factors (i.e., independent variables) that increase the likelihood of retention (or decrease the likelihood of churn)

* Retention (or Churn) is the outcome or dependent variable
    * DV = Binary, so Method = Logistic Regression
    
## Targeting Customers (Linear Regression) Example

### Overview

* Customer revenue, usage, and demographics for a cell phone provider
* DV:
    * Mean monthly revenue (prior 6 months), `avg6rev`
* IVs:
    * Mean monthly minutes (prior 6 months), `avg6mou`
    * Mean monthly customer care calls, `cc`
    * Mean monthly directory assistance calls, `da`
    * Mean monthly overage minutes, `ovrmou`
    * Household income (dollars), `income`
    * Own home (Yes; No), `own`
    
### Summarize Data

* Useful to examine data prior to specifying the model
    * Summary Statistics  

***Table 4.1: Summary Statistics***
``` {r t4summstat01, cache=TRUE, message=FALSE, warning=FALSE}
   # Use 'sumtable' from the 'vtable' package
sumtable(telecom)
```

***Figure 4.1: Scatterplot Matrix (with Correlations)***
```{r t4scatmat01, cache=TRUE, message=FALSE, warning=FALSE}
   # Use 'ggpairs' from the 'GGally' package
   # Pick only the numeric variables from the data using 'dplyr'
telecom %>%
   select(-own) %>%
   ggpairs(lower=list(continuous="smooth"),  # Ask for "regression" line
           diag=list(continuous="blankDiag"))  # Set diagonals to be blank
```

### Model Specification

* Goal: Determine what behaviors and demographics are associated with high revenue customers
* IVs are expected to be ones that are related to revenue
    * Expect an interaction between home ownership and overage minutes
* Model:
\begin{align}
avg6rev=\alpha + &\beta_1avg6mou + \beta_2cc + \\
&\beta_3da+\beta_4income + \\
&\beta_5ovrmou+\beta_6own + \beta_7(ovrmou\times own)
\end{align}

### Model Interpretation

#### Results

***Table 4.2: Linear Regression Results***
```{r t4lr01, cache=TRUE, message=FALSE, warning=FALSE, results="asis"}
   # Create model and save results to object
target <- lm(avg6rev ~ avg6mou + cc + da + income + ovrmou*own,
             data=telecom)
   # Use 'summ' from 'jtools' package for nicer output
summ(target, model.info=FALSE, digits=3)

```
\begin{align}
\hat{avg6rev} = 33.742 + &.049avg6mou-1.332cc + \\
&1.895da - .012income + \\
&.135ovrmou - 7.235own + 0.066(ovrmou\times own)
\end{align}

#### Testing Overall Model Significance

* Relationship between DV and combined effects of IVs
* $H_0: \text{all }\beta_k=0$ vs. $H_a: \text{at least one }\beta_k\ne0$
* Use F-statistic to test
* Conclusion:  With a F-statistic of $811.5$ and a $p<.001$, we conclude that at least one $\beta_k$ is significant

#### Assessing overall model fit

* How much variation in the DV is explained by the model
* Use $R^2$ to assess
* Use Adjusted $R^2$ to compare models
* Conclusion: Based on the $R^2$, about $70\%$ of the variance in `avg6rev` is explained by the model

#### Interpret Individual IVs

* Relationship between DV and each IV
* $H_0: \beta_k=0$ vs. $H_a: \beta_k\ne0$
* Interpret significant relationships
    * `avg6mou`
        * With $p<.001$, `avg6mou` has a significant effect on `avg6rev`.
        * A one unit increase in `avg6mou` is predicted to increase `avg6rev` by $.049$ units.
    * `cc`
        * With $p<.001$, `cc` has a significant effect on `avg6rev`.
        * A one unit increase in `cc` is predicted to decrease `avg6rev` by $1.332$ units.
    * `da`
        * With $p<.001$, `da` has a significant effect on `avg6rev`.
        * A one unit increase in `da` is predicted to increase `avg6rev` by $1.895$ units.
    * `ovrmou` and `own` interaction
        * With $p<.001$, the effect of `ovrmou` on `avg6rev` is significantly different based on `own`.
        * When the customer owns their home, a one unit increase in `ovrmou` is predicted to increase `avg6rev` by $.135+.066=.201$ units.
        * When the customer does notown their home, a one unit increase in `ovrmou` is predicted to increase `avg6rev` by $.135$ units.
    
* Sometimes helps to visually examine the IVs for interpretation
* Plots can show predicted DV at different levels of IVs

***Figure 4.2: Margin Plots for Significant IVs (No Interaction)***
```{r t4marginplots01, cache=TRUE, message=FALSE, warning=FALSE}
    # Use 'easy_mp' function from 'MKT4320BGSU' package
p1 <- easy_mp(target, focal="avg6mou", focal_range=c(0,1500))$plot +
      # Add layers to returned ggplot for nicer looking output
   ylim(30,120) +    # Consistent y-axis limits
   labs(x="Mean Monthly Minutes", y="Linear Prediction")
p2 <- easy_mp(target, focal="cc", focal_range=c(0,10))$plot +
      # Add layers to returned ggplot for nicer looking output
   ylim(30,120) +    # Consistent y-axis limits
   labs(x="Mean Monthly Customer Care Calls", y="Linear Prediction")
p3 <- easy_mp(target, focal="da", focal_range=c(0,10))$plot +
      # Add layers to returned ggplot for nicer looking output
   ylim(30,120) +    # Consistent y-axis limits
   labs(x="Mean Monthly Directory Assistance Calls", y="Linear Prediction")
   
   # Use 'plot_grid' from 'cowplot' package to combine plots
   # Need 'cowplot::' to force it to use plot_grid from 'cowplot' (vs. 'sjPlot')
cowplot::plot_grid(p1, p2, p3)
```

***Figure 4.3: Margin Plots for Interaction***
```{r t4marginplots01b, cache=TRUE, message=FALSE, warning=FALSE}
   # Use 'easy_mp' function from 'MKT4320BGSU' package
p4 <- easy_mp(target, focal="ovrmou", int="own", focal_range=c(0,300))$plot +
      # Add layers to returned ggplot for nicer looking output
   ylim(30,120) +    # Consistent y-axis limits
   labs(x="Mean Monthly Overage Minutes", y="Linear Prediction",
        color="Homeowner", fill="Homeowner")
p5 <- easy_mp(target, focal="own", int="ovrmou", int_range=c(0,300))$plot +
      # Add layers to returned ggplot for nicer looking output
   ylim(30,120) +    # Consistent y-axis limits
   labs(x="Home Ownership", y="Linear Prediction")

   # Use 'plot_grid' from 'cowplot' package to combine plots
cowplot::plot_grid(p4, p5, nrow=1)
```

***Figure 4.4: Margin Plot for Insignificant IV***
```{r t4marginplots02, cache=TRUE, message=FALSE, warning=FALSE}
    # Use 'easy_mp' function from 'MKT4320BGSU' package
easy_mp(target, focal="income")$plot +
      # Add layers to returned ggplot for nicer looking output
   ylim(30,120) +    # Consistent y-axis limits
   labs(x="Mean Monthly Minutes", y="Linear Prediction")
```       
* Examine deciles of predicted values by the IVs
    * Split sample into 10 groups based on predicted DV
    * Look at mean values of IVs for each decile  
    
***Table 4.3: IVs by Predicted Deciles***
```{r t4tgtdecile, cache=TRUE, message=FALSE, warning=FALSE}
telecom %>%
   cbind(., yhat=fitted(target)) %>%  # Append fitted values to data
   mutate(yhat.dec=11-ntile(yhat, 10),  # Create deciles, but reverse order
          own=as.numeric(own)-1) %>%  # Covert own to 1=yes, 0=no
   group_by(yhat.dec) %>%  # Group by decile
   summarise(across(avg6rev:own, 
                    ~mean(.x, na.rm=TRUE))) %>%  # Calculate mean of each IV
   flextable()  # Nice table
```

### Conclusion

Recall our Goal:<br>
Determine what behaviors and demographics are associated with high revenue customers

What did we learn?

* We can identify our highest revenue customers by examining `avg6mou`, `cc`, `da`, `ovrmou` and `own`
* Our highest revenue customers consumer over $1000$ minutes per month and have over $200$ overage minutes per month
* More directory assistance calls and fewer customer care calls are associated with higher revenue

## Targeting Customers (Logistic Regression)

### Overview

* Bank marketing data for customers of a bank
* DV:
    * Open term deposit account, `response`
* IVs:
    * Age, `age`
    * Average Yearly Balance, `balance`
    * Housing Loan (Yes, No), `housing`
    * Personal Loan (Yes, No), `loan`
    * Married (Yes, No), `married`
* Predict current customers likely to buy
    * Use training (75%) and holdout (25%) samples

### Estimation Results
    
***Table 4.4a: Logistic Regression Results on Training Sample (Logit Estimates)***
``` {r t4lrestimation, cache=TRUE, message=FALSE, warning=FALSE, resuts="asis"}
   # Use 'splitsample' function to create training and test data
sp <- splitsample(bankmktg, "response", seed = 9999)
train <- sp$train
test <- sp$test

   # Estimate using training sample
tgt.log <- glm(response ~ age + balance + housing + loan + married, 
               data=train, family="binomial")
   # Use 'summ' from 'jtools' for nice looking output
summ(tgt.log, digits=3, model.info=FALSE)

```

***Table 4.4b: Logistic Regression Results on Training Sample (Odds Ratio Estimates)***
``` {r t4lrestimation2, cache=TRUE, message=FALSE, warning=FALSE, resuts="asis"}
   # Use option 'exp=TRUE' for odds ratio estimates
summ(tgt.log, digits=3, model.info=FALSE, confint=FALSE,
     exp=TRUE, model.fit=FALSE)
```

### Overall Model Fit

* Based on the likelihood ratio test with p-value < .0001, the overall model is significant
* McFadden's Pseudo-$R^2$ of .026 means that the model explains only about 3% of variation between buyers/non-buyers
* Classification Matrix for Training Sample
* What's the problem?

***Table 4.5: Classification Matrix for Training Sample***
```{r t4cmtrain1, cache=TRUE, message=FALSE, warning=FALSE}
# NOTE: Saving as result for formatting of online lecture notes

cmout <- classify_logistic(MOD=tgt.log,          # Object with model results
                           DATA=train,         # Model data frame
                           POSITIVE="Yes",     # Factor level for "True"
                           DATA2=test,         # Test/holdout data frame
                           LABEL1="Training Data",
                           LABEL2="Test Data",
                           ft=TRUE)
cmout$sample1$table
```

* Sensitivity/Specificity Plot

***Figure 4.5: Sensitivity/Specificity Plot for Training Sample***
```{r t4sensspec01, cache=TRUE, message=FALSE, warning=FALSE}
    # Use'cutoff_logisit' function on training sample
sensspec <- cutoff_logistic(MOD=tgt.log, DATA=train, POSITIVE="Yes",
                LABEL1="Training Sample", DATA2=test, LABEL2="Test Sample",
                auto_print=FALSE)  # Only needed for online lecture materials
sensspec$sample1
```

* Classification Matrix for Training Sample (0.1 Cutoff)

***Table 4.5: Classification Matrix for Training Sample***
```{r t4cmtrain2, cache=TRUE, message=FALSE, warning=FALSE}
# NOTE: Saving as result for formatting of online lecture notes

cmout <- classify_logistic(MOD=tgt.log,          # Object with model results
                           DATA=train,         # Model data frame
                           POSITIVE="Yes",     # Factor level for "True"
                           CUTOFF=0.1,         # Use 0.1 for cutoff
                           DATA2=test,         # Test/holdout data frame
                           LABEL1="Training Data",
                           LABEL2="Test Data",
                           ft=TRUE)
cmout$sample1$table
```

* Classification Matrix for Holdout Sample (0.1 Cutoff)
    * Results very similar for holdout sample

***Table 4.6: Classification Matrix for Holdout Sample with 0.1 Cutoff***
```{r t4cmtrain3, cache=TRUE, message=FALSE, warning=FALSE}
cmout$sample2$table
```

* ROC Curve for Holdout Sample
    * Area between $.5$ and $.7$ suggests poor model fit

***Figure 4.6: ROC Curve for Holdout Sample***
```{r t4roctest1, cache=TRUE, message=FALSE, warning=FALSE}
rocout <- roc_logistic(MOD=tgt.log,          # Object with model results
                       DATA=train,         # Model data frame
                       DATA2=test,         # Test/holdout data frame
                       LABEL1="Training Data",
                       LABEL2="Test Data")
rocout$sample2
```    

### Interpreting Coefficients

* `age` is positive ($OR>1$) and significant ($p=.011$)
    * $1$ year increase in age increases odds of buying by a factor of $1.017$ (or odds of buying increase by $1.7\%$)
* `married` is negative ($OR<1$) and significant ($p<.001$)
    * Being married decreases odds of buying by factor of $.578$ (or odds of buying decrease by $42.2\%$)
* `housing` is negative ($OR<1$) and significant ($p=.001$)
    * Having a home loan decreases odds of buying by factor of $.627$ (or odds of buying decrease by $37.3\%$)
* `loan` is negative ($OR<1$) and significant ($p=.004$)
    * Having a personal loan decreases odds of buying by factor of $.527$ (or odds of buying decrease by $47.3\%$)

### Interpreting Coefficients Visually

***Figure 4.7: Margin Plots for Significant IVs***
```{r t4lrmarginplots, cache=TRUE, message=FALSE, warning=FALSE}
   # Use 'easy_mp' to create margin plots (with added layers for formatting)
lr1 <- easy_mp(tgt.log, focal="age")$plot +
   ylim(0,.3) +
   labs(x="Age", y="Pr(Response)")
lr2 <- easy_mp(tgt.log, focal="married")$plot +
   ylim(0,.3) +
   labs(x="Married", y="Pr(Response)")
lr3 <- easy_mp(tgt.log, focal="loan")$plot +
   ylim(0,.3) +
   labs(x="Personal Loan", y="Pr(Response)")
lr4 <- easy_mp(tgt.log, focal="housing")$plot +
   ylim(0,.3) +
   labs(x="Housing Loan", y="Pr(Response)")

   # Use 'plot_grid' to combine
cowplot::plot_grid(lr1, lr2, lr3, lr4, nrow=2)
```

### Gain Chart

* Contacting top $20\%$ of predicted buyers yields about $34\%$ of actual buyers
* Contacting top $30\%$ of predicted buyers yields about $45\%$ of actual buyers

***Figure 4.8: Gain Chart***
```{r t4gainlift01, cache=TRUE, message=FALSE, warning=FALSE}
   # Use 'gainlift_logistic' for gain and lift plots
glresults <- gainlift_logistic(tgt.log,  # Name of the glm results object
                               train,  # Name of the training data frame
                               test,   # Name of the testing data frame
                               "Yes")  # Level that represents success/true
    
   # Output gainplot
glresults$gainplot
```

### Lift Chart

* Contacting top $20\%$ of predicted buyers provides a lift of about $1.7$

***Figure 4.9: Lift Chart***
```{r t4gainlift02, cache=TRUE, message=FALSE, warning=FALSE}
   # Output liftplot
glresults$liftplot
```

### Conclusion

Recall our goal:

* Predict current customers likely to buy

What did we learn?

* We can identify those more likely to buy by examining `age`, `married`, `housing`, and `loan`
* By targeting those customers that are more likely to purchase, we can better spend our limited resources

## Suggested Readings

* *Marketing Data Science* (2015). Miller, Thomas W.
     * BGSU Library Link:<br><a href="https://librarysearch.bgsu.edu/permalink/01OHIOLINK_BGSU/i5ro6c/alma9926167769408506" target="_blank" rel="noopener noreferrer">https://librarysearch.bgsu.edu/permalink/01OHIOLINK_BGSU/i5ro6c/alma9926167769408506</a>
    * Chapter 3: Targeting Current Customers
